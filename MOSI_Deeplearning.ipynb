{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29e7bc68"
      },
      "source": [
        "## ðŸ“¦ Imports et Utilitaires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-WDiZf516i60"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error\n",
        "from scipy.stats import pearsonr\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebaa0930"
      },
      "source": [
        "## Ã‰TAPE 1: AmÃ©lioration de l'Encodage Textuel (X_t)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8P2gM10gQW8w"
      },
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encodeur de texte enrichi utilisant un Transformer Encoder.\n",
        "    Conserve les T=8 premiers tokens pour former X_t.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim=768, num_layers=2, num_heads=8, T=8):\n",
        "        super(TextEncoder, self).__init__()\n",
        "        self.T = T\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Token\n",
        "        self.em_token = nn.Parameter(torch.randn(1, 1, hidden_dim))\n",
        "\n",
        "        # Transformer Encoder Layer\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_dim * 4,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, bert_hidden_states):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            bert_hidden_states: [B, seq_len, hidden_dim] sÃ©quence brute de BERT\n",
        "        Returns:\n",
        "            X_t: [B, T, hidden_dim] sÃ©quence enrichie\n",
        "        \"\"\"\n",
        "        batch_size = bert_hidden_states.size(0)\n",
        "\n",
        "        # Ajouter le token spÃ©cial Em en tÃªte\n",
        "        em_expanded = self.em_token.expand(batch_size, -1, -1)\n",
        "        sequence = torch.cat([em_expanded, bert_hidden_states], dim=1)\n",
        "\n",
        "        # Appliquer le Transformer Encoder\n",
        "        encoded_sequence = self.transformer_encoder(sequence)\n",
        "\n",
        "        # Conserver uniquement les T premiers tokens\n",
        "        X_t = encoded_sequence[:, :self.T, :]\n",
        "\n",
        "        return X_t"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "badadeb4"
      },
      "source": [
        "## Ã‰TAPES 2 & 3: Descriptions Ã‰motionnelles (D_a et D_v)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZlW0zQG56Ww"
      },
      "source": [
        "def extract_audio_features(audio_data_vector):\n",
        "    \"\"\"\n",
        "    Extracts simplified audio features (Pitch, Loudness, Jitter, Shimmer)\n",
        "    from a pre-extracted audio feature vector, with a more meaningful conceptual mapping.\n",
        "    Args:\n",
        "        audio_data_vector: tensor [B, audio_dim] of audio features.\n",
        "    Returns:\n",
        "        dict with the 4 values (tensors [B]).\n",
        "    \"\"\"\n",
        "    batch_size = audio_data_vector.size(0)\n",
        "    audio_dim = audio_data_vector.size(1)\n",
        "\n",
        "\n",
        "    # Pitch: derived from mean of first 10 dimensions, normalized\n",
        "    pitch = audio_data_vector[:, :min(10, audio_dim)].mean(dim=1).abs() / 2 + 0.2\n",
        "    # Loudness: derived from mean of next 10 dimensions, normalized\n",
        "    loudness = audio_data_vector[:, min(10, audio_dim):min(20, audio_dim)].mean(dim=1).abs() / 2 + 0.2\n",
        "    # Jitter: derived from std dev of a segment, indicating variability\n",
        "    jitter = audio_data_vector[:, min(20, audio_dim):min(30, audio_dim)].std(dim=1) * 0.1\n",
        "    # Shimmer: derived from range of a segment\n",
        "    shimmer = (audio_data_vector[:, min(30, audio_dim):min(40, audio_dim)].max(dim=1).values -\n",
        "               audio_data_vector[:, min(30, audio_dim):min(40, audio_dim)].min(dim=1).values).abs() * 0.05\n",
        "\n",
        "    # Clamp values to a reasonable range\n",
        "    pitch = torch.clamp(pitch, 0.0, 1.0)\n",
        "    loudness = torch.clamp(loudness, 0.0, 1.0)\n",
        "    jitter = torch.clamp(jitter, 0.0, 0.5)\n",
        "    shimmer = torch.clamp(shimmer, 0.0, 0.5)\n",
        "\n",
        "    features = {\n",
        "        'pitch': pitch.cpu().numpy(),\n",
        "        'loudness': loudness.cpu().numpy(),\n",
        "        'jitter': jitter.cpu().numpy(),\n",
        "        'shimmer': shimmer.cpu().numpy()\n",
        "    }\n",
        "    return features\n",
        "\n",
        "\n",
        "def audio_description(pitch, loudness, jitter, shimmer):\n",
        "    \"\"\"\n",
        "    Generates a textual description of audio characteristics.\n",
        "    Args:\n",
        "        pitch, loudness, jitter, shimmer: scalar values\n",
        "    Returns:\n",
        "        str: descriptive sentence\n",
        "    \"\"\"\n",
        "    pitch_level = \"high\" if pitch > 0.6 else (\"low\" if pitch < 0.4 else \"normal\")\n",
        "    loudness_level = \"loud\" if loudness > 0.6 else (\"soft\" if loudness < 0.4 else \"moderate\")\n",
        "    jitter_level = \"unstable\" if jitter > 0.2 else \"stable\"\n",
        "    shimmer_level = \"varying\" if shimmer > 0.2 else \"consistent\"\n",
        "\n",
        "    description = (f\"The speaker used {pitch_level} pitch with {loudness_level} volume. \"\n",
        "                   f\"The voice quality is {jitter_level} with {shimmer_level} intensity.\")\n",
        "\n",
        "    return description\n",
        "\n",
        "\n",
        "def visual_description(visual_data_vector):\n",
        "    \"\"\"\n",
        "    Generates a textual description of facial Action Units, with a more\n",
        "    meaningful conceptual mapping from the simulated feature vector.\n",
        "    Args:\n",
        "        visual_data_vector: tensor [B, vision_dim] of visual features.\n",
        "    Returns:\n",
        "        str: descriptive sentence\n",
        "    \"\"\"\n",
        "    au_meanings = {\n",
        "        1: \"raised inner brow\", 2: \"raised outer brow\", 4: \"lowered brow\",\n",
        "        5: \"wide eyes\", 6: \"raised cheeks (smile)\", 7: \"tightened eyelids (joy/discomfort)\",\n",
        "        9: \"wrinkled nose (disgust)\", 12: \"lip corners pulled (smile)\", 15: \"depressed lip corners (sadness)\",\n",
        "        17: \"raised chin (contempt)\", 20: \"stretched lips\", 25: \"parted lips\", 26: \"jaw drop (surprise)\"\n",
        "    }\n",
        "\n",
        "    if visual_data_vector.dim() == 1:\n",
        "        visual_data_vector = visual_data_vector.unsqueeze(0)\n",
        "\n",
        "    batch_size = visual_data_vector.size(0)\n",
        "    vision_dim = visual_data_vector.size(1)\n",
        "\n",
        "    descriptions_batch = []\n",
        "    for i in range(batch_size):\n",
        "\n",
        "        active_aus_indices = []\n",
        "\n",
        "\n",
        "        if vision_dim >= 5:\n",
        "            if visual_data_vector[i, 0:5].mean() > 0.6: active_aus_indices.append(6) # Cheek raiser (smile)\n",
        "            if visual_data_vector[i, 5:10].mean() < -0.4: active_aus_indices.append(15) # Lip corner depressor (sadness)\n",
        "            if visual_data_vector[i, 10:15].max() > 0.7: active_aus_indices.append(26) # Jaw drop (surprise)\n",
        "            if visual_data_vector[i, 15:20].mean() > 0.5 and visual_data_vector[i, 20:25].mean() > 0.5: active_aus_indices.append(12) # Lip corner puller (smile)\n",
        "\n",
        "        current_descriptions = [au_meanings[au] for au in active_aus_indices if au in au_meanings]\n",
        "\n",
        "        if current_descriptions:\n",
        "            description = \"The person shows signs of: \" + \", \".join(current_descriptions) + \".\"\n",
        "        else:\n",
        "            description = \"The person shows a neutral facial expression.\"\n",
        "        descriptions_batch.append(description)\n",
        "\n",
        "    return descriptions_batch[0] if batch_size == 1 else descriptions_batch\n",
        "\n",
        "\n",
        "class EmotionalDescriptionEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encode les descriptions Ã©motionnelles textuelles en embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, bert_model, text_encoder):\n",
        "        super(EmotionalDescriptionEncoder, self).__init__()\n",
        "        self.bert_model = bert_model\n",
        "        self.text_encoder = text_encoder\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    def forward(self, descriptions, device):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            descriptions: liste de strings\n",
        "            device: torch device\n",
        "        Returns:\n",
        "            embeddings: [B, T, hidden_dim]\n",
        "        \"\"\"\n",
        "        # Tokenize les descriptions\n",
        "        encoded = self.tokenizer(\n",
        "            descriptions,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_tensors='pt'\n",
        "        ).to(device)\n",
        "\n",
        "        # Encoder avec BERT\n",
        "        with torch.no_grad():\n",
        "            bert_outputs = self.bert_model(**encoded)\n",
        "            hidden_states = bert_outputs.last_hidden_state\n",
        "\n",
        "        # Appliquer le TextEncoder\n",
        "        embeddings = self.text_encoder(hidden_states)\n",
        "\n",
        "        return embeddings"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f5b8016"
      },
      "source": [
        "## Ã‰TAPE 4: Attention CroisÃ©e et MFU (Minor Fusion Unit)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX0p8sJ19hS5"
      },
      "source": [
        "class CrossModalAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention CroisÃ©e Scaled Dot-Product pour la fusion text-guided.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim=768):\n",
        "        super(CrossModalAttention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.scale = hidden_dim ** -0.5\n",
        "\n",
        "        self.query_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.key_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.value_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, target_modality, text_guide):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            target_modality: [B, T, D] modalitÃ© Ã  enrichir (Audio ou Vision)\n",
        "            text_guide: [B, T, D] sÃ©quence textuelle X_t\n",
        "        Returns:\n",
        "            attended_features: [B, T, D] features enrichies\n",
        "        \"\"\"\n",
        "        Q = self.query_proj(target_modality)\n",
        "        K = self.key_proj(text_guide)\n",
        "        V = self.value_proj(text_guide)\n",
        "\n",
        "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        attended_features = torch.matmul(attention_weights, V)\n",
        "\n",
        "        return attended_features\n",
        "\n",
        "\n",
        "class MFU(nn.Module):\n",
        "    \"\"\"\n",
        "    Minor Fusion Unit: fusion rÃ©siduelle text-guided avec attention croisÃ©e.\n",
        "    Output = prev_fusion + Î±Â·Att_Tâ†’A(X_a, X_t) + Î²Â·Att_Tâ†’V(X_v, X_t)\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim=768):\n",
        "        super(MFU, self).__init__()\n",
        "\n",
        "        # Modules d'attention croisÃ©e\n",
        "        self.cross_attention_audio = CrossModalAttention(hidden_dim)\n",
        "        self.cross_attention_vision = CrossModalAttention(hidden_dim)\n",
        "\n",
        "        # Poids learnables Î± et Î²\n",
        "        self.alpha = nn.Parameter(torch.tensor(0.5))\n",
        "        self.beta = nn.Parameter(torch.tensor(0.5))\n",
        "\n",
        "        # Layer Norm pour stabilitÃ©\n",
        "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "    def forward(self, prev_fusion, X_a, X_v, X_t):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            prev_fusion: [B, T, D] fusion prÃ©cÃ©dente\n",
        "            X_a: [B, T, D] sÃ©quence audio\n",
        "            X_v: [B, T, D] sÃ©quence vision\n",
        "            X_t: [B, T, D] sÃ©quence texte (guide)\n",
        "        Returns:\n",
        "            fused_output: [B, T, D] fusion enrichie\n",
        "        \"\"\"\n",
        "        # Attention croisÃ©e guidÃ©e par le texte\n",
        "        attended_audio = self.cross_attention_audio(X_a, X_t)\n",
        "        attended_vision = self.cross_attention_vision(X_v, X_t)\n",
        "\n",
        "        # Fusion rÃ©siduelle avec poids learnables\n",
        "        fused_output = prev_fusion + self.alpha * attended_audio + self.beta * attended_vision\n",
        "\n",
        "        # Normalisation\n",
        "        fused_output = self.layer_norm(fused_output)\n",
        "\n",
        "        return fused_output\n",
        "\n",
        "\n",
        "class AudioVisualFeatureProjector(nn.Module):\n",
        "    \"\"\"\n",
        "    Projette les embeddings audio/vision en sÃ©quences alignÃ©es [B, T, D].\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim=768, T=8):\n",
        "        super(AudioVisualFeatureProjector, self).__init__()\n",
        "        self.T = T\n",
        "        self.projection = nn.Linear(input_dim, hidden_dim * T)\n",
        "        self.reshape_layer = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "\n",
        "    def forward(self, features):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            features: [B, input_dim] embeddings de modalitÃ©\n",
        "        Returns:\n",
        "            sequences: [B, T, hidden_dim]\n",
        "        \"\"\"\n",
        "        batch_size = features.size(0)\n",
        "\n",
        "        # Projeter et reshape en sÃ©quence\n",
        "        projected = self.projection(features)  # [B, hidden_dim * T]\n",
        "        sequences = projected.view(batch_size, self.T, -1)  # [B, T, hidden_dim]\n",
        "\n",
        "        # Normalisation\n",
        "        sequences = self.reshape_layer(sequences)\n",
        "\n",
        "        return sequences"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99b97c53"
      },
      "source": [
        "## Ã‰TAPE 5: Architecture DEVA ComplÃ¨te"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0sW21R9k3G4"
      },
      "source": [
        "class DEVANet(nn.Module):\n",
        "    \"\"\"\n",
        "    Architecture DEVA complÃ¨te pour l'analyse de sentiment multimodal.\n",
        "    \"\"\"\n",
        "    def __init__(self, audio_dim=128, vision_dim=512, hidden_dim=768, T=8, num_classes=1):\n",
        "        super(DEVANet, self).__init__()\n",
        "\n",
        "\n",
        "        # Encodeurs de base\n",
        "        self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.text_encoder = TextEncoder(hidden_dim=hidden_dim, T=T)\n",
        "\n",
        "        # Encodeur de descriptions Ã©motionnelles\n",
        "        self.emotion_encoder = EmotionalDescriptionEncoder(self.bert_model, self.text_encoder)\n",
        "\n",
        "        # Projecteurs pour audio et vision\n",
        "        self.audio_projector = AudioVisualFeatureProjector(audio_dim, hidden_dim, T)\n",
        "        self.vision_projector = AudioVisualFeatureProjector(vision_dim, hidden_dim, T)\n",
        "\n",
        "        # Projecteurs pour descriptions Ã©motionnelles\n",
        "        self.audio_desc_projector = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.vision_desc_projector = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        # Initial Fusion Layer (new)\n",
        "        self.initial_fusion_layer = nn.Linear(hidden_dim * 3, hidden_dim) # For concatenated X_t, X_a, X_v\n",
        "\n",
        "        # MFU (Minor Fusion Unit)\n",
        "        self.mfu = MFU(hidden_dim)\n",
        "\n",
        "        # Classificateur final\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * T, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim // 2, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, text_input, audio_features, vision_features,\n",
        "                audio_descriptions=None, visual_descriptions=None, device='cuda'):\n",
        "        \"\"\"\n",
        "        Forward pass complet de l'architecture DEVA.\n",
        "        \"\"\"\n",
        "        # 1. Encodage textuel enrichi (X_t)\n",
        "        bert_outputs = self.bert_model(**text_input)\n",
        "        X_t = self.text_encoder(bert_outputs.last_hidden_state)  # [B, T, D]\n",
        "\n",
        "        # 2. Projection des modalitÃ©s audio et vision en sÃ©quences\n",
        "        X_a = self.audio_projector(audio_features)   # [B, T, D]\n",
        "        X_v = self.vision_projector(vision_features) # [B, T, D]\n",
        "\n",
        "        # 3. IntÃ©gration des descriptions Ã©motionnelles (si disponibles)\n",
        "        if audio_descriptions is not None:\n",
        "            if isinstance(audio_descriptions, list):\n",
        "                D_a = self.emotion_encoder(audio_descriptions, device)\n",
        "            else: # Assume it's already an embedding tensor [B, T_desc, D]\n",
        "                D_a = audio_descriptions.to(device)\n",
        "\n",
        "            D_a_pooled = D_a.mean(dim=1)  # [B, D]\n",
        "            D_a_proj = self.audio_desc_projector(D_a_pooled).unsqueeze(1)  # [B, 1, D]\n",
        "            X_a = X_a + D_a_proj  # Enrichissement rÃ©siduel\n",
        "\n",
        "        if visual_descriptions is not None:\n",
        "            if isinstance(visual_descriptions, list):\n",
        "                D_v = self.emotion_encoder(visual_descriptions, device)\n",
        "            else: # Assume it's already an embedding tensor [B, T_desc, D]\n",
        "                D_v = visual_descriptions.to(device)\n",
        "\n",
        "            D_v_pooled = D_v.mean(dim=1)  # [B, D]\n",
        "            D_v_proj = self.vision_desc_projector(D_v_pooled).unsqueeze(1)  # [B, 1, D]\n",
        "            X_v = X_v + D_v_proj  # Enrichissement rÃ©siduel\n",
        "\n",
        "        # 4. Initial Multimodal Fusion (Enhanced)\n",
        "        # [B, T, D] -> [B, T, 3D] -> [B, T, D]\n",
        "        concatenated_features = torch.cat((X_t, X_a, X_v), dim=-1)\n",
        "        prev_fusion = self.initial_fusion_layer(concatenated_features)\n",
        "\n",
        "        # 5. Fusion text-guided via MFU\n",
        "        fused_output = self.mfu(prev_fusion, X_a, X_v, X_t)  # [B, T, D]\n",
        "\n",
        "        # 6. Pooling et classification\n",
        "        fused_flat = fused_output.view(fused_output.size(0), -1)  # [B, T*D]\n",
        "        logits = self.classifier(fused_flat)  # [B, 1]\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9w5x1K30xG4"
      },
      "source": [
        "def evaluate_metrics(labels, preds):\n",
        "    \"\"\"\n",
        "    Calcule les mÃ©triques acadÃ©miques standards pour l'analyse de sentiment.\n",
        "\n",
        "    Args:\n",
        "        labels: [N] valeurs rÃ©elles (sentiment scores)\n",
        "        preds: [N] prÃ©dictions du modÃ¨le\n",
        "\n",
        "    Returns:\n",
        "        dict: {'Acc-2': float, 'F1': float, 'MAE': float, 'Corr': float, 'Acc-5': float, 'Acc-7': float}\n",
        "    \"\"\"\n",
        "    # Conversion en numpy\n",
        "    if isinstance(labels, torch.Tensor):\n",
        "        labels = labels.cpu().numpy()\n",
        "    if isinstance(preds, torch.Tensor):\n",
        "        preds = preds.cpu().numpy()\n",
        "\n",
        "    # Acc-2: Accuracy binaire\n",
        "    labels_binary = (labels > 0).astype(int)\n",
        "    preds_binary = (preds > 0).astype(int)\n",
        "    acc_2 = accuracy_score(labels_binary, preds_binary)\n",
        "\n",
        "    # F1-score\n",
        "    f1 = f1_score(labels_binary, preds_binary, average='weighted', zero_division=0)\n",
        "\n",
        "    # MAE: Mean Absolute Error\n",
        "    mae = mean_absolute_error(labels, preds)\n",
        "\n",
        "    # CorrÃ©lation de Pearson\n",
        "    corr, _ = pearsonr(labels.flatten(), preds.flatten())\n",
        "\n",
        "    # Acc-5: Accuracy within\n",
        "    acc_5_mask = np.abs(labels - preds) < 0.5\n",
        "    acc_5 = np.mean(acc_5_mask)\n",
        "\n",
        "    # Acc-7: Accuracy within\n",
        "    acc_7_mask = np.abs(labels - preds) < 0.7\n",
        "    acc_7 = np.mean(acc_7_mask)\n",
        "\n",
        "\n",
        "    return {\n",
        "        'Acc-2': acc_2,\n",
        "        'F1': f1,\n",
        "        'MAE': mae,\n",
        "        'Corr': corr,\n",
        "        'Acc-5': acc_5,\n",
        "        'Acc-7': acc_7\n",
        "    }"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b9b8df9"
      },
      "source": [
        "## Dataset et DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX0p8sJ19hS6"
      },
      "source": [
        "class CMUMOSIDatasetConceptual(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset conceptuel imitant la structure de CMU-MOSI pour les tests.\n",
        "    GÃ©nÃ¨re des donnÃ©es synthÃ©tiques but avec les dimensions et types attendus\n",
        "    des features prÃ©-traitÃ©es de CMU-MOSI.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_samples=1000, audio_dim=74, vision_dim=35, text_max_length=64,\n",
        "                 bert_model=None, text_encoder=None, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.num_samples = num_samples\n",
        "        self.audio_dim = audio_dim\n",
        "        self.vision_dim = vision_dim\n",
        "        self.text_max_length = text_max_length\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.device = device\n",
        "\n",
        "        self.audio_embeddings = []\n",
        "        self.visual_embeddings = []\n",
        "\n",
        "        if bert_model and text_encoder:\n",
        "            self.emotion_encoder = EmotionalDescriptionEncoder(bert_model, text_encoder)\n",
        "        else:\n",
        "            self.emotion_encoder = None\n",
        "            print(\"Warning: BertModel and TextEncoder not provided to CMUMOSIDatasetConceptual. Emotional descriptions will not be pre-encoded.\")\n",
        "\n",
        "\n",
        "        # GÃ©nÃ©rer des donnÃ©es conceptuelles MOSI\n",
        "        self._generate_conceptual_mosi_data()\n",
        "        self._pre_encode_descriptions()\n",
        "\n",
        "    def _generate_conceptual_mosi_data(self):\n",
        "        \"\"\"\n",
        "        GÃ©nÃ¨re des donnÃ©es synthÃ©tiques mais structurÃ©es comme CMU-MOSI,\n",
        "        establishing a logical relationship between modalities and labels.\n",
        "        \"\"\"\n",
        "        print(\"Generating conceptual CMU-MOSI like data with ground truth relationships...\")\n",
        "        self.texts = []\n",
        "        self.audio_features = torch.zeros(self.num_samples, self.audio_dim)\n",
        "        self.visual_features = torch.zeros(self.num_samples, self.vision_dim)\n",
        "        self.labels = torch.zeros(self.num_samples)\n",
        "\n",
        "        for i in range(self.num_samples):\n",
        "            # Introduce a 'sentiment' factor for each sample\n",
        "            sentiment_factor = np.random.uniform(-1, 1)\n",
        "\n",
        "            # Generate audio features influenced by sentiment_factor\n",
        "            self.audio_features[i, :self.audio_dim//2] = torch.randn(self.audio_dim//2) * (1 + sentiment_factor * 0.5) # Magnitude based on sentiment\n",
        "            self.audio_features[i, self.audio_dim//2:] = torch.randn(self.audio_dim - self.audio_dim//2) * (1 - sentiment_factor * 0.3) # Variability based on sentiment\n",
        "\n",
        "            # Generate visual features influenced by sentiment_factor\n",
        "            if sentiment_factor > 0.3: # Positive sentiment\n",
        "                if self.vision_dim >= 5: # For AU6 (cheek raiser)\n",
        "                    self.visual_features[i, 0:5] = torch.rand(5) * 0.8 + 0.2 # Indicator for AU6 (smile)\n",
        "                if self.vision_dim >= 15: # For AU12 (smile)\n",
        "                    self.visual_features[i, 10:15] = torch.rand(5) * 0.7 + 0.3 # Indicator for AU12 (smile)\n",
        "            elif sentiment_factor < -0.3: # Negative sentiment\n",
        "                if self.vision_dim >= 10: # For AU15 (sadness)\n",
        "                    self.visual_features[i, 5:10] = torch.rand(5) * -0.8 - 0.2 # Indicator for AU15 (sadness)\n",
        "                if self.vision_dim >= 20: # For AU4 (frown)\n",
        "                    self.visual_features[i, 15:20] = torch.rand(5) * 0.6 + 0.1 # Indicator for AU4 (frown)\n",
        "            else: # Neutral\n",
        "                self.visual_features[i, :] = torch.randn(self.vision_dim) * 0.1\n",
        "\n",
        "            # Generate text based on sentiment_factor\n",
        "            if sentiment_factor > 0.5:\n",
        "                text_candidates = [\"This is absolutely fantastic!\", \"I loved every single moment.\", \"Such a wonderful experience.\", \"Truly amazing!\"]\n",
        "            elif sentiment_factor < -0.5:\n",
        "                text_candidates = [\"I am so disappointed.\", \"This is terrible and frustrating.\", \"What a horrible situation.\", \"Absolutely awful.\"]\n",
        "            else:\n",
        "                text_candidates = [\"It's okay, nothing special.\", \"I feel quite neutral about this.\", \"It was average.\", \"Could be better, could be worse.\"]\n",
        "            self.texts.append(np.random.choice(text_candidates))\n",
        "\n",
        "            # Labels (sentiment scores)\n",
        "            self.labels[i] = torch.tensor(sentiment_factor * 3.0 + np.random.uniform(-0.5, 0.5)) # Add some noise\n",
        "\n",
        "        self.labels = torch.clamp(self.labels, -3.0, 3.0) # MOSI range\n",
        "\n",
        "        print(f\"Generated {self.num_samples} samples of conceptual MOSI data with ground truth relationships.\")\n",
        "\n",
        "    def _pre_encode_descriptions(self):\n",
        "        \"\"\"\n",
        "        Pre-calculates and stores emotional description embeddings.\n",
        "        \"\"\"\n",
        "        if self.emotion_encoder is None:\n",
        "            print(\"Skipping pre-encoding descriptions as emotion_encoder is not initialized.\")\n",
        "            return\n",
        "\n",
        "        print(\"Pre-encoding emotional descriptions...\")\n",
        "        all_audio_descs = []\n",
        "        all_visual_descs = []\n",
        "\n",
        "        # Generate all descriptions\n",
        "        for i in range(self.num_samples):\n",
        "            audio_feats = extract_audio_features(self.audio_features[i].unsqueeze(0)) # Pass [1, audio_dim]\n",
        "            audio_desc = audio_description(\n",
        "                audio_feats['pitch'][0], audio_feats['loudness'][0],\n",
        "                audio_feats['jitter'][0], audio_feats['shimmer'][0]\n",
        "            )\n",
        "            all_audio_descs.append(audio_desc)\n",
        "\n",
        "            visual_desc = visual_description(self.visual_features[i].unsqueeze(0)) # Pass [1, vision_dim]\n",
        "            all_visual_descs.append(visual_desc)\n",
        "\n",
        "        # Encode all descriptions\n",
        "        batch_size = 32\n",
        "        num_batches = (self.num_samples + batch_size - 1) // batch_size\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            start_idx = i * batch_size\n",
        "            end_idx = min((i + 1) * batch_size, self.num_samples)\n",
        "\n",
        "            batch_audio_descs = all_audio_descs[start_idx:end_idx]\n",
        "            batch_visual_descs = all_visual_descs[start_idx:end_idx]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # The emotion_encoder returns [B, T, D]\n",
        "                audio_emb = self.emotion_encoder(batch_audio_descs, self.device)\n",
        "                visual_emb = self.emotion_encoder(batch_visual_descs, self.device)\n",
        "\n",
        "            self.audio_embeddings.append(audio_emb.cpu())\n",
        "            self.visual_embeddings.append(visual_emb.cpu())\n",
        "\n",
        "        self.audio_embeddings = torch.cat(self.audio_embeddings, dim=0) # Shape [num_samples, T_desc, hidden_dim]\n",
        "        self.visual_embeddings = torch.cat(self.visual_embeddings, dim=0) # Shape [num_samples, T_desc, hidden_dim]\n",
        "\n",
        "        print(\"Finished pre-encoding emotional descriptions.\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        audio_feature = self.audio_features[idx]\n",
        "        vision_feature = self.visual_features[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Tokenization\n",
        "        encoded = self.tokenizer(\n",
        "            text,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.text_max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        item = {\n",
        "            'input_ids': encoded['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoded['attention_mask'].squeeze(0),\n",
        "            'audio': audio_feature,\n",
        "            'vision': vision_feature,\n",
        "            'label': label\n",
        "        }\n",
        "\n",
        "        # Add pre-encoded embeddings if available\n",
        "        if self.audio_embeddings is not None and len(self.audio_embeddings) > 0:\n",
        "            item['audio_description_embedding'] = self.audio_embeddings[idx]\n",
        "            item['visual_description_embedding'] = self.visual_embeddings[idx]\n",
        "\n",
        "        return item"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encodeur de texte enrichi utilisant un Transformer Encoder.\n",
        "    Conserve les T=8 premiers tokens pour former X_t.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim=768, num_layers=2, num_heads=8, T=8):\n",
        "        super(TextEncoder, self).__init__()\n",
        "        self.T = T\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Token spÃ©cial Em (learnable)\n",
        "        self.em_token = nn.Parameter(torch.randn(1, 1, hidden_dim))\n",
        "\n",
        "        # Transformer Encoder Layer\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_dim * 4,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, bert_hidden_states):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            bert_hidden_states: [B, seq_len, hidden_dim] sÃ©quence brute de BERT\n",
        "        Returns:\n",
        "            X_t: [B, T, hidden_dim] sÃ©quence enrichie\n",
        "        \"\"\"\n",
        "        batch_size = bert_hidden_states.size(0)\n",
        "\n",
        "        # Ajouter le token spÃ©cial Em en tÃªte\n",
        "        em_expanded = self.em_token.expand(batch_size, -1, -1)\n",
        "        sequence = torch.cat([em_expanded, bert_hidden_states], dim=1)\n",
        "\n",
        "        # Appliquer le Transformer Encoder\n",
        "        encoded_sequence = self.transformer_encoder(sequence)\n",
        "\n",
        "        # Conserver uniquement les T premiers tokens\n",
        "        X_t = encoded_sequence[:, :self.T, :]\n",
        "\n",
        "        return X_t\n"
      ],
      "metadata": {
        "id": "3NNfPhVO6rFR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b4f4aa2"
      },
      "source": [
        "## ðŸš€ EntraÃ®nement et Ã‰valuation du ModÃ¨le"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_deva_model(num_epochs=5, batch_size=16, learning_rate=1e-4):\n",
        "    \"\"\"\n",
        "    EntraÃ®ne le modÃ¨le DEVA et Ã©value les performances.\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"ðŸš€ Utilisation du device: {device}\\n\")\n",
        "\n",
        "    # Initialize BERT and TextEncoder once for the dataset and model\n",
        "    bert_model_for_encoder = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
        "    text_encoder_for_dataset = TextEncoder(hidden_dim=768, T=8).to(device) # Assuming T=8 from DEVANet\n",
        "\n",
        "    # Dataset et DataLoader - Utilisation du dataset conceptuel CMU-MOSI\n",
        "    # Pass bert_model and text_encoder to pre-calculate descriptions\n",
        "    train_dataset = CMUMOSIDatasetConceptual(\n",
        "        num_samples=800,\n",
        "        audio_dim=74, vision_dim=35, text_max_length=64,\n",
        "        bert_model=bert_model_for_encoder, text_encoder=text_encoder_for_dataset, device=device\n",
        "    )\n",
        "    test_dataset = CMUMOSIDatasetConceptual(\n",
        "        num_samples=200,\n",
        "        audio_dim=74, vision_dim=35, text_max_length=64,\n",
        "        bert_model=bert_model_for_encoder, text_encoder=text_encoder_for_dataset, device=device\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # ModÃ¨le (DEVANet initializes its own BERT and TextEncoder for X_t)\n",
        "    model = DEVANet(audio_dim=74, vision_dim=35, hidden_dim=768, T=8, num_classes=1)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Optimiseur et Loss\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    print(\"\\n============================= Training Start =============================\\n\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train() # Set model to training mode\n",
        "        total_loss = 0\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Move data to device\n",
        "            text_input = {'input_ids': batch['input_ids'].to(device), 'attention_mask': batch['attention_mask'].to(device)}\n",
        "            audio_features = batch['audio'].to(device)\n",
        "            vision_features = batch['vision'].to(device)\n",
        "            labels = batch['label'].float().to(device).unsqueeze(1) # Ensure labels are float and have correct shape\n",
        "\n",
        "            # Emotional descriptions are pre-encoded as embeddings in the dataset\n",
        "            audio_desc_emb = batch['audio_description_embedding'].to(device) if 'audio_description_embedding' in batch else None\n",
        "            visual_desc_emb = batch['visual_description_embedding'].to(device) if 'visual_description_embedding' in batch else None\n",
        "\n",
        "            outputs = model(text_input, audio_features, vision_features, audio_desc_emb, visual_desc_emb, device=device)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Evaluation loop\n",
        "        model.eval() # Set model to evaluation mode\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                text_input = {'input_ids': batch['input_ids'].to(device), 'attention_mask': batch['attention_mask'].to(device)}\n",
        "                audio_features = batch['audio'].to(device)\n",
        "                vision_features = batch['vision'].to(device)\n",
        "                labels = batch['label'].float().to(device).unsqueeze(1)\n",
        "\n",
        "                audio_desc_emb = batch['audio_description_embedding'].to(device) if 'audio_description_embedding' in batch else None\n",
        "                visual_desc_emb = batch['visual_description_embedding'].to(device) if 'visual_description_embedding' in batch else None\n",
        "\n",
        "                outputs = model(text_input, audio_features, vision_features, audio_desc_emb, visual_desc_emb, device=device)\n",
        "\n",
        "                all_preds.extend(outputs.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        all_preds = np.array(all_preds).flatten()\n",
        "        all_labels = np.array(all_labels).flatten()\n",
        "\n",
        "        # Calculate and print metrics\n",
        "        metrics = evaluate_metrics(all_labels, all_preds)\n",
        "        print(f\"Evaluation - MAE: {metrics['MAE']:.4f}, Corr: {metrics['Corr']:.4f}, Acc-2: {metrics['Acc-2']:.4f}, F1: {metrics['F1']:.4f}\")\n",
        "\n",
        "    print(\"\\n============================= Training End =============================\\n\")\n",
        "    return metrics\n",
        "\n",
        "metrics = train_deva_model(num_epochs=5, batch_size=16, learning_rate=1e-4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oweKyq-FIAbQ",
        "outputId": "bf63db7d-7848-4777-cc44-13ca7dbfcabb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Utilisation du device: cpu\n",
            "\n",
            "Generating conceptual CMU-MOSI like data with ground truth relationships...\n",
            "Generated 800 samples of conceptual MOSI data with ground truth relationships.\n",
            "Pre-encoding emotional descriptions...\n",
            "Finished pre-encoding emotional descriptions.\n",
            "Generating conceptual CMU-MOSI like data with ground truth relationships...\n",
            "Generated 200 samples of conceptual MOSI data with ground truth relationships.\n",
            "Pre-encoding emotional descriptions...\n",
            "Finished pre-encoding emotional descriptions.\n",
            "\n",
            "============================= Training Start =============================\n",
            "\n",
            "Epoch 1/5, Train Loss: 0.6987\n",
            "Evaluation - MAE: 0.5105, Corr: 0.9525, Acc-2: 0.8950, F1: 0.8944\n",
            "Epoch 2/5, Train Loss: 0.3857\n",
            "Evaluation - MAE: 0.5263, Corr: 0.9591, Acc-2: 0.8750, F1: 0.8741\n",
            "Epoch 3/5, Train Loss: 0.3936\n",
            "Evaluation - MAE: 0.4551, Corr: 0.9593, Acc-2: 0.8750, F1: 0.8750\n",
            "Epoch 4/5, Train Loss: 0.3828\n",
            "Evaluation - MAE: 0.4490, Corr: 0.9554, Acc-2: 0.8750, F1: 0.8747\n",
            "Epoch 5/5, Train Loss: 0.3494\n",
            "Evaluation - MAE: 0.4641, Corr: 0.9570, Acc-2: 0.8850, F1: 0.8851\n",
            "\n",
            "============================= Training End =============================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9383eea4"
      },
      "source": [
        "## ðŸ“Š Visualisation des RÃ©sultats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b85abb4d"
      },
      "source": [
        "## MÃ©triques d'Ã‰valuation AcadÃ©miques"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "metrics_data = metrics.copy()\n",
        "for key, value in metrics_data.items():\n",
        "    if isinstance(value, np.float32):\n",
        "        metrics_data[key] = float(value)\n",
        "\n",
        "\n",
        "labels = list(metrics_data.keys())\n",
        "values = list(metrics_data.values())\n",
        "\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "plt.bar(labels, values, color=['skyblue', 'lightcoral', 'lightgreen', 'gold', 'orange', 'purple'])\n",
        "plt.xlabel('Metric')\n",
        "plt.ylabel('Value')\n",
        "plt.title('DEVANet Model Evaluation Metrics')\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "for i, value in enumerate(values):\n",
        "    plt.text(i, value + 0.02, f'{value:.4f}', ha='center', va='bottom')\n",
        "\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "zLXvB0WGIPxF",
        "outputId": "7ab1f92a-efe9-4d4c-9b4b-03864ccaf0d2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}
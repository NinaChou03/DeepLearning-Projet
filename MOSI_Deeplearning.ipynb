{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29e7bc68"
      },
      "source": [
        "## ðŸ“¦ Imports et Utilitaires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-WDiZf516i60"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error\n",
        "from scipy.stats import pearsonr\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebaa0930"
      },
      "source": [
        "## Ã‰TAPE 1: AmÃ©lioration de l'Encodage Textuel (X_t)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8P2gM10gQW8w"
      },
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encodeur de texte enrichi utilisant un Transformer Encoder.\n",
        "    Conserve les T=8 premiers tokens pour former X_t.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim=768, num_layers=2, num_heads=8, T=8):\n",
        "        super(TextEncoder, self).__init__()\n",
        "        self.T = T\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Token spÃ©cial Em (learnable)\n",
        "        self.em_token = nn.Parameter(torch.randn(1, 1, hidden_dim))\n",
        "\n",
        "        # Transformer Encoder Layer\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_dim * 4,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, bert_hidden_states):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            bert_hidden_states: [B, seq_len, hidden_dim] sÃ©quence brute de BERT\n",
        "        Returns:\n",
        "            X_t: [B, T, hidden_dim] sÃ©quence enrichie\n",
        "        \"\"\"\n",
        "        batch_size = bert_hidden_states.size(0)\n",
        "\n",
        "        # Ajouter le token spÃ©cial Em en tÃªte\n",
        "        em_expanded = self.em_token.expand(batch_size, -1, -1)\n",
        "        sequence = torch.cat([em_expanded, bert_hidden_states], dim=1)\n",
        "\n",
        "        # Appliquer le Transformer Encoder\n",
        "        encoded_sequence = self.transformer_encoder(sequence)\n",
        "\n",
        "        # Conserver uniquement les T premiers tokens\n",
        "        X_t = encoded_sequence[:, :self.T, :]\n",
        "\n",
        "        return X_t"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b85abb4d"
      },
      "source": [
        "## MÃ©triques d'Ã‰valuation AcadÃ©miques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "badadeb4"
      },
      "source": [
        "## Ã‰TAPES 2 & 3: Descriptions Ã‰motionnelles (D_a et D_v)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZlW0zQG56Ww"
      },
      "source": [
        "def extract_audio_features(audio_data_vector):\n",
        "    \"\"\"\n",
        "    Extracts simplified audio features (Pitch, Loudness, Jitter, Shimmer)\n",
        "    from a pre-extracted audio feature vector, with a more meaningful conceptual mapping.\n",
        "    Args:\n",
        "        audio_data_vector: tensor [B, audio_dim] of audio features.\n",
        "    Returns:\n",
        "        dict with the 4 values (tensors [B]).\n",
        "    \"\"\"\n",
        "    batch_size = audio_data_vector.size(0)\n",
        "    audio_dim = audio_data_vector.size(1)\n",
        "\n",
        "    # Conceptual mapping: assume certain parts of the vector 'indicate' certain LLDs\n",
        "    # Make them more sensitive to the *values* within the vector, not just indices.\n",
        "    # For instance, higher average values in a segment could mean higher pitch/loudness.\n",
        "\n",
        "    # Simulating values based on sub-sections of the input vector\n",
        "    # Let's assume the first few dims relate to pitch, next to loudness, etc.\n",
        "    # These are still conceptual but try to react to the input data's magnitude/pattern.\n",
        "\n",
        "    # Pitch: derived from mean of first 10 dimensions, normalized\n",
        "    pitch = audio_data_vector[:, :min(10, audio_dim)].mean(dim=1).abs() / 2 + 0.2\n",
        "    # Loudness: derived from mean of next 10 dimensions, normalized\n",
        "    loudness = audio_data_vector[:, min(10, audio_dim):min(20, audio_dim)].mean(dim=1).abs() / 2 + 0.2\n",
        "    # Jitter: derived from std dev of a segment, indicating variability\n",
        "    jitter = audio_data_vector[:, min(20, audio_dim):min(30, audio_dim)].std(dim=1) * 0.1\n",
        "    # Shimmer: derived from range of a segment\n",
        "    shimmer = (audio_data_vector[:, min(30, audio_dim):min(40, audio_dim)].max(dim=1).values -\n",
        "               audio_data_vector[:, min(30, audio_dim):min(40, audio_dim)].min(dim=1).values).abs() * 0.05\n",
        "\n",
        "    # Clamp values to a reasonable range\n",
        "    pitch = torch.clamp(pitch, 0.0, 1.0)\n",
        "    loudness = torch.clamp(loudness, 0.0, 1.0)\n",
        "    jitter = torch.clamp(jitter, 0.0, 0.5)\n",
        "    shimmer = torch.clamp(shimmer, 0.0, 0.5)\n",
        "\n",
        "    features = {\n",
        "        'pitch': pitch.cpu().numpy(),\n",
        "        'loudness': loudness.cpu().numpy(),\n",
        "        'jitter': jitter.cpu().numpy(),\n",
        "        'shimmer': shimmer.cpu().numpy()\n",
        "    }\n",
        "    return features\n",
        "\n",
        "\n",
        "def audio_description(pitch, loudness, jitter, shimmer):\n",
        "    \"\"\"\n",
        "    Generates a textual description of audio characteristics.\n",
        "    Args:\n",
        "        pitch, loudness, jitter, shimmer: scalar values\n",
        "    Returns:\n",
        "        str: descriptive sentence\n",
        "    \"\"\"\n",
        "    pitch_level = \"high\" if pitch > 0.6 else (\"low\" if pitch < 0.4 else \"normal\")\n",
        "    loudness_level = \"loud\" if loudness > 0.6 else (\"soft\" if loudness < 0.4 else \"moderate\")\n",
        "    jitter_level = \"unstable\" if jitter > 0.2 else \"stable\"\n",
        "    shimmer_level = \"varying\" if shimmer > 0.2 else \"consistent\"\n",
        "\n",
        "    description = (f\"The speaker used {pitch_level} pitch with {loudness_level} volume. \"\n",
        "                   f\"The voice quality is {jitter_level} with {shimmer_level} intensity.\")\n",
        "\n",
        "    return description\n",
        "\n",
        "\n",
        "def visual_description(visual_data_vector):\n",
        "    \"\"\"\n",
        "    Generates a textual description of facial Action Units, with a more\n",
        "    meaningful conceptual mapping from the simulated feature vector.\n",
        "    Args:\n",
        "        visual_data_vector: tensor [B, vision_dim] of visual features.\n",
        "    Returns:\n",
        "        str: descriptive sentence\n",
        "    \"\"\"\n",
        "    au_meanings = {\n",
        "        1: \"raised inner brow\", 2: \"raised outer brow\", 4: \"lowered brow\",\n",
        "        5: \"wide eyes\", 6: \"raised cheeks (smile)\", 7: \"tightened eyelids (joy/discomfort)\",\n",
        "        9: \"wrinkled nose (disgust)\", 12: \"lip corners pulled (smile)\", 15: \"depressed lip corners (sadness)\",\n",
        "        17: \"raised chin (contempt)\", 20: \"stretched lips\", 25: \"parted lips\", 26: \"jaw drop (surprise)\"\n",
        "    }\n",
        "\n",
        "    if visual_data_vector.dim() == 1:\n",
        "        visual_data_vector = visual_data_vector.unsqueeze(0)\n",
        "\n",
        "    batch_size = visual_data_vector.size(0)\n",
        "    vision_dim = visual_data_vector.size(1)\n",
        "\n",
        "    descriptions_batch = []\n",
        "    for i in range(batch_size):\n",
        "        # Conceptual mapping: associate feature vector segments with AUs\n",
        "        # Active AUs are determined by comparing values to a threshold,\n",
        "        # making them responsive to the synthetic feature patterns.\n",
        "        active_aus_indices = []\n",
        "\n",
        "        # Example: if the average of a segment is high, trigger an AU\n",
        "        # This is still conceptual but provides a more responsive simulation.\n",
        "        # Max 3-4 AUs for brevity\n",
        "        if vision_dim >= 5: # Ensure enough dimensions\n",
        "            if visual_data_vector[i, 0:5].mean() > 0.6: active_aus_indices.append(6) # Cheek raiser (smile)\n",
        "            if visual_data_vector[i, 5:10].mean() < -0.4: active_aus_indices.append(15) # Lip corner depressor (sadness)\n",
        "            if visual_data_vector[i, 10:15].max() > 0.7: active_aus_indices.append(26) # Jaw drop (surprise)\n",
        "            if visual_data_vector[i, 15:20].mean() > 0.5 and visual_data_vector[i, 20:25].mean() > 0.5: active_aus_indices.append(12) # Lip corner puller (smile)\n",
        "\n",
        "        current_descriptions = [au_meanings[au] for au in active_aus_indices if au in au_meanings]\n",
        "\n",
        "        if current_descriptions:\n",
        "            description = \"The person shows signs of: \" + \", \".join(current_descriptions) + \".\"\n",
        "        else:\n",
        "            description = \"The person shows a neutral facial expression.\"\n",
        "        descriptions_batch.append(description)\n",
        "\n",
        "    return descriptions_batch[0] if batch_size == 1 else descriptions_batch\n",
        "\n",
        "\n",
        "class EmotionalDescriptionEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encode les descriptions Ã©motionnelles textuelles en embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, bert_model, text_encoder):\n",
        "        super(EmotionalDescriptionEncoder, self).__init__()\n",
        "        self.bert_model = bert_model\n",
        "        self.text_encoder = text_encoder\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    def forward(self, descriptions, device):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            descriptions: liste de strings\n",
        "            device: torch device\n",
        "        Returns:\n",
        "            embeddings: [B, T, hidden_dim]\n",
        "        \"\"\"\n",
        "        # Tokenize les descriptions\n",
        "        encoded = self.tokenizer(\n",
        "            descriptions,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_tensors='pt'\n",
        "        ).to(device)\n",
        "\n",
        "        # Encoder avec BERT\n",
        "        with torch.no_grad():\n",
        "            bert_outputs = self.bert_model(**encoded)\n",
        "            hidden_states = bert_outputs.last_hidden_state\n",
        "\n",
        "        # Appliquer le TextEncoder\n",
        "        embeddings = self.text_encoder(hidden_states)\n",
        "\n",
        "        return embeddings"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f5b8016"
      },
      "source": [
        "## Ã‰TAPE 4: Attention CroisÃ©e et MFU (Minor Fusion Unit)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX0p8sJ19hS5"
      },
      "source": [
        "class CrossModalAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention CroisÃ©e Scaled Dot-Product pour la fusion text-guided.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim=768):\n",
        "        super(CrossModalAttention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.scale = hidden_dim ** -0.5\n",
        "\n",
        "        self.query_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.key_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.value_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, target_modality, text_guide):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            target_modality: [B, T, D] modalitÃ© Ã  enrichir (Audio ou Vision)\n",
        "            text_guide: [B, T, D] sÃ©quence textuelle X_t\n",
        "        Returns:\n",
        "            attended_features: [B, T, D] features enrichies\n",
        "        \"\"\"\n",
        "        # Q vient de la modalitÃ© cible, K et V du texte\n",
        "        Q = self.query_proj(target_modality)  # [B, T, D]\n",
        "        K = self.key_proj(text_guide)         # [B, T, D]\n",
        "        V = self.value_proj(text_guide)       # [B, T, D]\n",
        "\n",
        "        # Attention: Q @ K^T / sqrt(d)\n",
        "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale  # [B, T, T]\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        # Appliquer l'attention sur V\n",
        "        attended_features = torch.matmul(attention_weights, V)  # [B, T, D]\n",
        "\n",
        "        return attended_features\n",
        "\n",
        "\n",
        "class MFU(nn.Module):\n",
        "    \"\"\"\n",
        "    Minor Fusion Unit: fusion rÃ©siduelle text-guided avec attention croisÃ©e.\n",
        "    Output = prev_fusion + Î±Â·Att_Tâ†’A(X_a, X_t) + Î²Â·Att_Tâ†’V(X_v, X_t)\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim=768):\n",
        "        super(MFU, self).__init__()\n",
        "\n",
        "        # Modules d'attention croisÃ©e\n",
        "        self.cross_attention_audio = CrossModalAttention(hidden_dim)\n",
        "        self.cross_attention_vision = CrossModalAttention(hidden_dim)\n",
        "\n",
        "        # Poids learnables Î± et Î²\n",
        "        self.alpha = nn.Parameter(torch.tensor(0.5))\n",
        "        self.beta = nn.Parameter(torch.tensor(0.5))\n",
        "\n",
        "        # Layer Norm pour stabilitÃ©\n",
        "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "    def forward(self, prev_fusion, X_a, X_v, X_t):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            prev_fusion: [B, T, D] fusion prÃ©cÃ©dente\n",
        "            X_a: [B, T, D] sÃ©quence audio\n",
        "            X_v: [B, T, D] sÃ©quence vision\n",
        "            X_t: [B, T, D] sÃ©quence texte (guide)\n",
        "        Returns:\n",
        "            fused_output: [B, T, D] fusion enrichie\n",
        "        \"\"\"\n",
        "        # Attention croisÃ©e guidÃ©e par le texte\n",
        "        attended_audio = self.cross_attention_audio(X_a, X_t)\n",
        "        attended_vision = self.cross_attention_vision(X_v, X_t)\n",
        "\n",
        "        # Fusion rÃ©siduelle avec poids learnables\n",
        "        fused_output = prev_fusion + self.alpha * attended_audio + self.beta * attended_vision\n",
        "\n",
        "        # Normalisation\n",
        "        fused_output = self.layer_norm(fused_output)\n",
        "\n",
        "        return fused_output\n",
        "\n",
        "\n",
        "class AudioVisualFeatureProjector(nn.Module):\n",
        "    \"\"\"\n",
        "    Projette les embeddings audio/vision en sÃ©quences alignÃ©es [B, T, D].\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim=768, T=8):\n",
        "        super(AudioVisualFeatureProjector, self).__init__()\n",
        "        self.T = T\n",
        "        self.projection = nn.Linear(input_dim, hidden_dim * T)\n",
        "        self.reshape_layer = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "\n",
        "    def forward(self, features):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            features: [B, input_dim] embeddings de modalitÃ©\n",
        "        Returns:\n",
        "            sequences: [B, T, hidden_dim]\n",
        "        \"\"\"\n",
        "        batch_size = features.size(0)\n",
        "\n",
        "        # Projeter et reshape en sÃ©quence\n",
        "        projected = self.projection(features)  # [B, hidden_dim * T]\n",
        "        sequences = projected.view(batch_size, self.T, -1)  # [B, T, hidden_dim]\n",
        "\n",
        "        # Normalisation\n",
        "        sequences = self.reshape_layer(sequences)\n",
        "\n",
        "        return sequences"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99b97c53"
      },
      "source": [
        "## Ã‰TAPE 5: Architecture DEVA ComplÃ¨te"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0sW21R9k3G4"
      },
      "source": [
        "class DEVANet(nn.Module):\n",
        "    \"\"\"\n",
        "    Architecture DEVA complÃ¨te pour l'analyse de sentiment multimodal.\n",
        "    \"\"\"\n",
        "    def __init__(self, audio_dim=128, vision_dim=512, hidden_dim=768, T=8, num_classes=1):\n",
        "        super(DEVANet, self).__init__()\n",
        "\n",
        "\n",
        "        # Encodeurs de base\n",
        "        self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.text_encoder = TextEncoder(hidden_dim=hidden_dim, T=T)\n",
        "\n",
        "        # Encodeur de descriptions Ã©motionnelles\n",
        "        self.emotion_encoder = EmotionalDescriptionEncoder(self.bert_model, self.text_encoder)\n",
        "\n",
        "        # Projecteurs pour audio et vision\n",
        "        self.audio_projector = AudioVisualFeatureProjector(audio_dim, hidden_dim, T)\n",
        "        self.vision_projector = AudioVisualFeatureProjector(vision_dim, hidden_dim, T)\n",
        "\n",
        "        # Projecteurs pour descriptions Ã©motionnelles\n",
        "        self.audio_desc_projector = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.vision_desc_projector = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        # Initial Fusion Layer (new)\n",
        "        self.initial_fusion_layer = nn.Linear(hidden_dim * 3, hidden_dim) # For concatenated X_t, X_a, X_v\n",
        "\n",
        "        # MFU (Minor Fusion Unit)\n",
        "        self.mfu = MFU(hidden_dim)\n",
        "\n",
        "        # Classificateur final\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * T, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim // 2, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, text_input, audio_features, vision_features,\n",
        "                audio_descriptions=None, visual_descriptions=None, device='cuda'):\n",
        "        \"\"\"\n",
        "        Forward pass complet de l'architecture DEVA.\n",
        "        \"\"\"\n",
        "        # 1. Encodage textuel enrichi (X_t)\n",
        "        bert_outputs = self.bert_model(**text_input)\n",
        "        X_t = self.text_encoder(bert_outputs.last_hidden_state)  # [B, T, D]\n",
        "\n",
        "        # 2. Projection des modalitÃ©s audio et vision en sÃ©quences\n",
        "        X_a = self.audio_projector(audio_features)   # [B, T, D]\n",
        "        X_v = self.vision_projector(vision_features) # [B, T, D]\n",
        "\n",
        "        # 3. IntÃ©gration des descriptions Ã©motionnelles (si disponibles)\n",
        "        if audio_descriptions is not None:\n",
        "            # If descriptions are already embeddings, they come in as [B, T_desc, D]\n",
        "            # If they are strings, they come in as list of strings\n",
        "            if isinstance(audio_descriptions, list):\n",
        "                D_a = self.emotion_encoder(audio_descriptions, device)\n",
        "            else: # Assume it's already an embedding tensor [B, T_desc, D]\n",
        "                D_a = audio_descriptions.to(device)\n",
        "\n",
        "            D_a_pooled = D_a.mean(dim=1)  # [B, D]\n",
        "            D_a_proj = self.audio_desc_projector(D_a_pooled).unsqueeze(1)  # [B, 1, D]\n",
        "            X_a = X_a + D_a_proj  # Enrichissement rÃ©siduel\n",
        "\n",
        "        if visual_descriptions is not None:\n",
        "            if isinstance(visual_descriptions, list):\n",
        "                D_v = self.emotion_encoder(visual_descriptions, device)\n",
        "            else: # Assume it's already an embedding tensor [B, T_desc, D]\n",
        "                D_v = visual_descriptions.to(device)\n",
        "\n",
        "            D_v_pooled = D_v.mean(dim=1)  # [B, D]\n",
        "            D_v_proj = self.vision_desc_projector(D_v_pooled).unsqueeze(1)  # [B, 1, D]\n",
        "            X_v = X_v + D_v_proj  # Enrichissement rÃ©siduel\n",
        "\n",
        "        # 4. Initial Multimodal Fusion (Enhanced)\n",
        "        # Concatenate X_t, X_a, and X_v along a new dimension and pass through a linear layer\n",
        "        # [B, T, D] -> [B, T, 3D] -> [B, T, D]\n",
        "        concatenated_features = torch.cat((X_t, X_a, X_v), dim=-1)\n",
        "        prev_fusion = self.initial_fusion_layer(concatenated_features)\n",
        "\n",
        "        # 5. Fusion text-guided via MFU\n",
        "        fused_output = self.mfu(prev_fusion, X_a, X_v, X_t)  # [B, T, D]\n",
        "\n",
        "        # 6. Pooling et classification\n",
        "        fused_flat = fused_output.view(fused_output.size(0), -1)  # [B, T*D]\n",
        "        logits = self.classifier(fused_flat)  # [B, 1]\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9w5x1K30xG4"
      },
      "source": [
        "def evaluate_metrics(labels, preds):\n",
        "    \"\"\"\n",
        "    Calcule les mÃ©triques acadÃ©miques standards pour l'analyse de sentiment.\n",
        "\n",
        "    Args:\n",
        "        labels: [N] valeurs rÃ©elles (sentiment scores)\n",
        "        preds: [N] prÃ©dictions du modÃ¨le\n",
        "\n",
        "    Returns:\n",
        "        dict: {'Acc-2': float, 'F1': float, 'MAE': float, 'Corr': float, 'Acc-5': float, 'Acc-7': float}\n",
        "    \"\"\"\n",
        "    # Conversion en numpy\n",
        "    if isinstance(labels, torch.Tensor):\n",
        "        labels = labels.cpu().numpy()\n",
        "    if isinstance(preds, torch.Tensor):\n",
        "        preds = preds.cpu().numpy()\n",
        "\n",
        "    # Acc-2: Accuracy binaire (Positif vs NÃ©gatif, seuil Ã  0)\n",
        "    labels_binary = (labels > 0).astype(int)\n",
        "    preds_binary = (preds > 0).astype(int)\n",
        "    acc_2 = accuracy_score(labels_binary, preds_binary)\n",
        "\n",
        "    # F1-score (Binaire, weighted)\n",
        "    f1 = f1_score(labels_binary, preds_binary, average='weighted', zero_division=0)\n",
        "\n",
        "    # MAE: Mean Absolute Error\n",
        "    mae = mean_absolute_error(labels, preds)\n",
        "\n",
        "    # CorrÃ©lation de Pearson\n",
        "    corr, _ = pearsonr(labels.flatten(), preds.flatten())\n",
        "\n",
        "    # Acc-5: Accuracy within +/- 0.5\n",
        "    acc_5_mask = np.abs(labels - preds) < 0.5\n",
        "    acc_5 = np.mean(acc_5_mask)\n",
        "\n",
        "    # Acc-7: Accuracy within +/- 0.7\n",
        "    acc_7_mask = np.abs(labels - preds) < 0.7\n",
        "    acc_7 = np.mean(acc_7_mask)\n",
        "\n",
        "\n",
        "    return {\n",
        "        'Acc-2': acc_2,\n",
        "        'F1': f1,\n",
        "        'MAE': mae,\n",
        "        'Corr': corr,\n",
        "        'Acc-5': acc_5,\n",
        "        'Acc-7': acc_7\n",
        "    }"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b9b8df9"
      },
      "source": [
        "## Dataset et DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX0p8sJ19hS6"
      },
      "source": [
        "class CMUMOSIDatasetConceptual(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset conceptuel imitant la structure de CMU-MOSI pour les tests.\n",
        "    GÃ©nÃ¨re des donnÃ©es synthÃ©tiques but avec les dimensions et types attendus\n",
        "    des features prÃ©-traitÃ©es de CMU-MOSI.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_samples=1000, audio_dim=74, vision_dim=35, text_max_length=64,\n",
        "                 bert_model=None, text_encoder=None, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.num_samples = num_samples\n",
        "        self.audio_dim = audio_dim\n",
        "        self.vision_dim = vision_dim\n",
        "        self.text_max_length = text_max_length\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.device = device\n",
        "\n",
        "        self.audio_embeddings = []\n",
        "        self.visual_embeddings = []\n",
        "\n",
        "        if bert_model and text_encoder:\n",
        "            self.emotion_encoder = EmotionalDescriptionEncoder(bert_model, text_encoder)\n",
        "        else:\n",
        "            self.emotion_encoder = None\n",
        "            print(\"Warning: BertModel and TextEncoder not provided to CMUMOSIDatasetConceptual. Emotional descriptions will not be pre-encoded.\")\n",
        "\n",
        "\n",
        "        # GÃ©nÃ©rer des donnÃ©es conceptuelles MOSI\n",
        "        self._generate_conceptual_mosi_data()\n",
        "        self._pre_encode_descriptions() # New step to pre-encode\n",
        "\n",
        "    def _generate_conceptual_mosi_data(self):\n",
        "        \"\"\"\n",
        "        GÃ©nÃ¨re des donnÃ©es synthÃ©tiques mais structurÃ©es comme CMU-MOSI,\n",
        "        establishing a logical relationship between modalities and labels.\n",
        "        \"\"\"\n",
        "        print(\"Generating conceptual CMU-MOSI like data with ground truth relationships...\")\n",
        "        self.texts = []\n",
        "        self.audio_features = torch.zeros(self.num_samples, self.audio_dim)\n",
        "        self.visual_features = torch.zeros(self.num_samples, self.vision_dim)\n",
        "        self.labels = torch.zeros(self.num_samples)\n",
        "\n",
        "        for i in range(self.num_samples):\n",
        "            # Introduce a 'sentiment' factor for each sample\n",
        "            sentiment_factor = np.random.uniform(-1, 1) # Range [-1, 1]\n",
        "\n",
        "            # Generate audio features influenced by sentiment_factor\n",
        "            # e.g., positive sentiment -> higher pitch/loudness indicators\n",
        "            # negative sentiment -> lower pitch/loudness indicators, more jitter/shimmer\n",
        "            self.audio_features[i, :self.audio_dim//2] = torch.randn(self.audio_dim//2) * (1 + sentiment_factor * 0.5) # Magnitude based on sentiment\n",
        "            self.audio_features[i, self.audio_dim//2:] = torch.randn(self.audio_dim - self.audio_dim//2) * (1 - sentiment_factor * 0.3) # Variability based on sentiment\n",
        "\n",
        "            # Generate visual features influenced by sentiment_factor\n",
        "            # e.g., positive sentiment -> features indicating smiles (AU6, AU12)\n",
        "            # negative sentiment -> features indicating frowns (AU4, AU15)\n",
        "            # We'll make specific segments of the vector indicative of AUs\n",
        "            if sentiment_factor > 0.3: # Positive sentiment\n",
        "                # Ensure the indices are within vision_dim bounds\n",
        "                if self.vision_dim >= 5: # For AU6 (cheek raiser)\n",
        "                    self.visual_features[i, 0:5] = torch.rand(5) * 0.8 + 0.2 # Indicator for AU6 (smile)\n",
        "                if self.vision_dim >= 15: # For AU12 (smile)\n",
        "                    self.visual_features[i, 10:15] = torch.rand(5) * 0.7 + 0.3 # Indicator for AU12 (smile)\n",
        "            elif sentiment_factor < -0.3: # Negative sentiment\n",
        "                if self.vision_dim >= 10: # For AU15 (sadness)\n",
        "                    self.visual_features[i, 5:10] = torch.rand(5) * -0.8 - 0.2 # Indicator for AU15 (sadness)\n",
        "                if self.vision_dim >= 20: # For AU4 (frown)\n",
        "                    self.visual_features[i, 15:20] = torch.rand(5) * 0.6 + 0.1 # Indicator for AU4 (frown)\n",
        "            else: # Neutral\n",
        "                self.visual_features[i, :] = torch.randn(self.vision_dim) * 0.1 # Small random noise\n",
        "\n",
        "            # Generate text based on sentiment_factor\n",
        "            if sentiment_factor > 0.5:\n",
        "                text_candidates = [\"This is absolutely fantastic!\", \"I loved every single moment.\", \"Such a wonderful experience.\", \"Truly amazing!\"]\n",
        "            elif sentiment_factor < -0.5:\n",
        "                text_candidates = [\"I am so disappointed.\", \"This is terrible and frustrating.\", \"What a horrible situation.\", \"Absolutely awful.\"]\n",
        "            else:\n",
        "                text_candidates = [\"It's okay, nothing special.\", \"I feel quite neutral about this.\", \"It was average.\", \"Could be better, could be worse.\"]\n",
        "            self.texts.append(np.random.choice(text_candidates))\n",
        "\n",
        "            # Labels (sentiment scores) derived from sentiment_factor, scaled to [-3, 3]\n",
        "            self.labels[i] = torch.tensor(sentiment_factor * 3.0 + np.random.uniform(-0.5, 0.5)) # Add some noise\n",
        "\n",
        "        self.labels = torch.clamp(self.labels, -3.0, 3.0) # Ensure labels are within MOSI range\n",
        "\n",
        "        print(f\"Generated {self.num_samples} samples of conceptual MOSI data with ground truth relationships.\")\n",
        "\n",
        "    def _pre_encode_descriptions(self):\n",
        "        \"\"\"\n",
        "        Pre-calculates and stores emotional description embeddings.\n",
        "        \"\"\"\n",
        "        if self.emotion_encoder is None:\n",
        "            print(\"Skipping pre-encoding descriptions as emotion_encoder is not initialized.\")\n",
        "            return\n",
        "\n",
        "        print(\"Pre-encoding emotional descriptions...\")\n",
        "        all_audio_descs = []\n",
        "        all_visual_descs = []\n",
        "\n",
        "        # Generate all descriptions first using the refined functions\n",
        "        for i in range(self.num_samples):\n",
        "            audio_feats = extract_audio_features(self.audio_features[i].unsqueeze(0)) # Pass [1, audio_dim]\n",
        "            audio_desc = audio_description(\n",
        "                audio_feats['pitch'][0], audio_feats['loudness'][0],\n",
        "                audio_feats['jitter'][0], audio_feats['shimmer'][0]\n",
        "            )\n",
        "            all_audio_descs.append(audio_desc)\n",
        "\n",
        "            visual_desc = visual_description(self.visual_features[i].unsqueeze(0)) # Pass [1, vision_dim]\n",
        "            all_visual_descs.append(visual_desc)\n",
        "\n",
        "        # Encode all descriptions in batches to leverage GPU if available\n",
        "        batch_size = 32 # Can be adjusted\n",
        "        num_batches = (self.num_samples + batch_size - 1) // batch_size\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            start_idx = i * batch_size\n",
        "            end_idx = min((i + 1) * batch_size, self.num_samples)\n",
        "\n",
        "            batch_audio_descs = all_audio_descs[start_idx:end_idx]\n",
        "            batch_visual_descs = all_visual_descs[start_idx:end_idx]\n",
        "\n",
        "            # Use the emotion_encoder to get embeddings\n",
        "            with torch.no_grad():\n",
        "                # The emotion_encoder returns [B, T, D] where T is the sequence length of the description\n",
        "                audio_emb = self.emotion_encoder(batch_audio_descs, self.device)\n",
        "                visual_emb = self.emotion_encoder(batch_visual_descs, self.device)\n",
        "\n",
        "            # We extend with actual tensors, not lists of tensors\n",
        "            self.audio_embeddings.append(audio_emb.cpu())\n",
        "            self.visual_embeddings.append(visual_emb.cpu())\n",
        "\n",
        "        # Concatenate all batched embeddings into a single tensor list\n",
        "        self.audio_embeddings = torch.cat(self.audio_embeddings, dim=0) # Shape [num_samples, T_desc, hidden_dim]\n",
        "        self.visual_embeddings = torch.cat(self.visual_embeddings, dim=0) # Shape [num_samples, T_desc, hidden_dim]\n",
        "\n",
        "        print(\"Finished pre-encoding emotional descriptions.\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        audio_feature = self.audio_features[idx]\n",
        "        vision_feature = self.visual_features[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Tokenization\n",
        "        encoded = self.tokenizer(\n",
        "            text,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.text_max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        item = {\n",
        "            'input_ids': encoded['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoded['attention_mask'].squeeze(0),\n",
        "            'audio': audio_feature,\n",
        "            'vision': vision_feature,\n",
        "            'label': label\n",
        "        }\n",
        "\n",
        "        # Add pre-encoded embeddings if available\n",
        "        if self.audio_embeddings is not None and len(self.audio_embeddings) > 0:\n",
        "            item['audio_description_embedding'] = self.audio_embeddings[idx]\n",
        "            item['visual_description_embedding'] = self.visual_embeddings[idx]\n",
        "\n",
        "        return item"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encodeur de texte enrichi utilisant un Transformer Encoder.\n",
        "    Conserve les T=8 premiers tokens pour former X_t.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim=768, num_layers=2, num_heads=8, T=8):\n",
        "        super(TextEncoder, self).__init__()\n",
        "        self.T = T\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Token spÃ©cial Em (learnable)\n",
        "        self.em_token = nn.Parameter(torch.randn(1, 1, hidden_dim))\n",
        "\n",
        "        # Transformer Encoder Layer\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_dim * 4,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, bert_hidden_states):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            bert_hidden_states: [B, seq_len, hidden_dim] sÃ©quence brute de BERT\n",
        "        Returns:\n",
        "            X_t: [B, T, hidden_dim] sÃ©quence enrichie\n",
        "        \"\"\"\n",
        "        batch_size = bert_hidden_states.size(0)\n",
        "\n",
        "        # Ajouter le token spÃ©cial Em en tÃªte\n",
        "        em_expanded = self.em_token.expand(batch_size, -1, -1)\n",
        "        sequence = torch.cat([em_expanded, bert_hidden_states], dim=1)\n",
        "\n",
        "        # Appliquer le Transformer Encoder\n",
        "        encoded_sequence = self.transformer_encoder(sequence)\n",
        "\n",
        "        # Conserver uniquement les T premiers tokens\n",
        "        X_t = encoded_sequence[:, :self.T, :]\n",
        "\n",
        "        return X_t\n"
      ],
      "metadata": {
        "id": "3NNfPhVO6rFR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b4f4aa2"
      },
      "source": [
        "## ðŸš€ EntraÃ®nement et Ã‰valuation du ModÃ¨le"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_deva_model(num_epochs=5, batch_size=16, learning_rate=1e-4):\n",
        "    \"\"\"\n",
        "    EntraÃ®ne le modÃ¨le DEVA et Ã©value les performances.\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"ðŸš€ Utilisation du device: {device}\\n\")\n",
        "\n",
        "    # Initialize BERT and TextEncoder once for the dataset and model\n",
        "    bert_model_for_encoder = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
        "    text_encoder_for_dataset = TextEncoder(hidden_dim=768, T=8).to(device) # Assuming T=8 from DEVANet\n",
        "\n",
        "    # Dataset et DataLoader - Utilisation du dataset conceptuel CMU-MOSI\n",
        "    # Pass bert_model and text_encoder to pre-calculate descriptions\n",
        "    train_dataset = CMUMOSIDatasetConceptual(\n",
        "        num_samples=800,\n",
        "        audio_dim=74, vision_dim=35, text_max_length=64,\n",
        "        bert_model=bert_model_for_encoder, text_encoder=text_encoder_for_dataset, device=device\n",
        "    )\n",
        "    test_dataset = CMUMOSIDatasetConceptual(\n",
        "        num_samples=200,\n",
        "        audio_dim=74, vision_dim=35, text_max_length=64,\n",
        "        bert_model=bert_model_for_encoder, text_encoder=text_encoder_for_dataset, device=device\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # ModÃ¨le (DEVANet initializes its own BERT and TextEncoder for X_t)\n",
        "    model = DEVANet(audio_dim=74, vision_dim=35, hidden_dim=768, T=8, num_classes=1)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Optimiseur et Loss\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    print(\"\\n============================= Training Start =============================\\n\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train() # Set model to training mode\n",
        "        total_loss = 0\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Move data to device\n",
        "            text_input = {'input_ids': batch['input_ids'].to(device), 'attention_mask': batch['attention_mask'].to(device)}\n",
        "            audio_features = batch['audio'].to(device)\n",
        "            vision_features = batch['vision'].to(device)\n",
        "            labels = batch['label'].float().to(device).unsqueeze(1) # Ensure labels are float and have correct shape\n",
        "\n",
        "            # Emotional descriptions are pre-encoded as embeddings in the dataset\n",
        "            audio_desc_emb = batch['audio_description_embedding'].to(device) if 'audio_description_embedding' in batch else None\n",
        "            visual_desc_emb = batch['visual_description_embedding'].to(device) if 'visual_description_embedding' in batch else None\n",
        "\n",
        "            outputs = model(text_input, audio_features, vision_features, audio_desc_emb, visual_desc_emb, device=device)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Evaluation loop\n",
        "        model.eval() # Set model to evaluation mode\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                text_input = {'input_ids': batch['input_ids'].to(device), 'attention_mask': batch['attention_mask'].to(device)}\n",
        "                audio_features = batch['audio'].to(device)\n",
        "                vision_features = batch['vision'].to(device)\n",
        "                labels = batch['label'].float().to(device).unsqueeze(1)\n",
        "\n",
        "                audio_desc_emb = batch['audio_description_embedding'].to(device) if 'audio_description_embedding' in batch else None\n",
        "                visual_desc_emb = batch['visual_description_embedding'].to(device) if 'visual_description_embedding' in batch else None\n",
        "\n",
        "                outputs = model(text_input, audio_features, vision_features, audio_desc_emb, visual_desc_emb, device=device)\n",
        "\n",
        "                all_preds.extend(outputs.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        all_preds = np.array(all_preds).flatten()\n",
        "        all_labels = np.array(all_labels).flatten()\n",
        "\n",
        "        # Calculate and print metrics\n",
        "        metrics = evaluate_metrics(all_labels, all_preds)\n",
        "        print(f\"Evaluation - MAE: {metrics['MAE']:.4f}, Corr: {metrics['Corr']:.4f}, Acc-2: {metrics['Acc-2']:.4f}, F1: {metrics['F1']:.4f}\")\n",
        "\n",
        "    print(\"\\n============================= Training End =============================\\n\")\n",
        "    return metrics\n",
        "\n",
        "metrics = train_deva_model(num_epochs=5, batch_size=16, learning_rate=1e-4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oweKyq-FIAbQ",
        "outputId": "903a2a72-2533-48e4-b70e-fd67ce79f142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Utilisation du device: cpu\n",
            "\n",
            "Generating conceptual CMU-MOSI like data with ground truth relationships...\n",
            "Generated 800 samples of conceptual MOSI data with ground truth relationships.\n",
            "Pre-encoding emotional descriptions...\n",
            "Finished pre-encoding emotional descriptions.\n",
            "Generating conceptual CMU-MOSI like data with ground truth relationships...\n",
            "Generated 200 samples of conceptual MOSI data with ground truth relationships.\n",
            "Pre-encoding emotional descriptions...\n",
            "Finished pre-encoding emotional descriptions.\n",
            "\n",
            "============================= Training Start =============================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9383eea4"
      },
      "source": [
        "## ðŸ“Š Visualisation des RÃ©sultats"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'metrics' dictionary is available from the previous run\n",
        "# If you run this cell independently, ensure 'metrics' is defined.\n",
        "metrics_data = metrics.copy()\n",
        "# Convert numpy.float32 to standard float for plotting if present\n",
        "for key, value in metrics_data.items():\n",
        "    if isinstance(value, np.float32):\n",
        "        metrics_data[key] = float(value)\n",
        "\n",
        "\n",
        "labels = list(metrics_data.keys())\n",
        "values = list(metrics_data.values())\n",
        "\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "plt.bar(labels, values, color=['skyblue', 'lightcoral', 'lightgreen', 'gold', 'orange', 'purple'])\n",
        "plt.xlabel('Metric')\n",
        "plt.ylabel('Value')\n",
        "plt.title('DEVANet Model Evaluation Metrics')\n",
        "plt.ylim(0, 1) # Metrics like accuracy, F1, correlation are usually between 0 and 1\n",
        "\n",
        "# Annotate bars with their values\n",
        "for i, value in enumerate(values):\n",
        "    plt.text(i, value + 0.02, f'{value:.4f}', ha='center', va='bottom')\n",
        "\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zLXvB0WGIPxF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}